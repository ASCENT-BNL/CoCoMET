{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This path append is just for testing from within the examples folder, no need for the user to add this\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONFIG String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_str = \"\"\"\n",
    "# SETUP VARIABLES: These determine basic CoMET functionality\n",
    "verbose: True # Whether to use verbose output\n",
    "parallel_processing: True # [bool] Whether or not to use parallel processing for certain tasks\n",
    "max_cores: 32 # Number of cores to use if parallel_processing==True; Enter None for unlimited\n",
    "\n",
    "# Structered in this form:\n",
    "# Observation Type:\n",
    "#   path_to_data\n",
    "#   additional_observation_parameters\n",
    "#\n",
    "#   tracker:\n",
    "#       tracker_params\n",
    "#\n",
    "#       analysis:\n",
    "#           analysis_variables\n",
    "\n",
    "# WRF\n",
    "wrf:\n",
    "    path_to_data: \"/share/D3/data/hweiner/WRF/wrfout_6_19_2013/*\"\n",
    "    \n",
    "    is_idealized: False\n",
    "\n",
    "    feature_tracking_var: \"dbz\" #DBZ, TB, WA, PR, Or other WRF Standard variable name (Case-Sensitive)\n",
    "    segmentation_var: \"dbz\"\n",
    "\n",
    "    tobac:\n",
    "        feature_id:\n",
    "            threshold: [30, 40, 50]\n",
    "            target: \"maximum\"\n",
    "            position_threshold: \"weighted_diff\"\n",
    "            sigma_threshold: 0.5\n",
    "            n_min_threshold: 4\n",
    "\n",
    "        linking:\n",
    "            method_linking: \"predict\"\n",
    "            adaptive_stop: 0.2\n",
    "            adaptive_step: 0.95\n",
    "            order: 1\n",
    "            subnetwork_size: 10\n",
    "            memory: 0\n",
    "            v_max: 20\n",
    "            time_cell_min: 120\n",
    "\n",
    "        segmentation_2d:\n",
    "            height: 2 # km\n",
    "            method: \"watershed\"\n",
    "            target: 'maximum'\n",
    "            threshold: 30\n",
    "\n",
    "        segmentation_3d:\n",
    "            method: \"watershed\"\n",
    "            target: 'maximum'\n",
    "            threshold: 30\n",
    "            seed_3D_flag: 'box'\n",
    "    \n",
    "        analysis: # Optional\n",
    "            merge_split: { variable: \"DBZ\" }\n",
    "            eth: { variable: \"DBZ\", threshold : 30}\n",
    "            area-low: { height: 2 }\n",
    "            area-high: { height: 6,}\n",
    "            volume: {}\n",
    "            # volume-high: { threshold: 30 }\n",
    "            max_intensity: { variable : \"DBZ\"}\n",
    "            # max_height: { variable : \"DBZ\", threshold : 30}\n",
    "            velocity: {}\n",
    "            perimeter: {}\n",
    "            cell_growth: {}\n",
    "            irregularity: {irregularity_metrics: ['convexity', 'sphericity']}\n",
    "\n",
    "    tams:\n",
    "        ctt_threshold: 235\n",
    "        ctt_core_threshold: 219\n",
    "        u_projection: 0\n",
    "        parallel: False\n",
    "\n",
    "        analysis_type: \"cloud\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run CoMET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====WRF Setup Found in CONFIG=====\n",
      "{'verbose': True, 'parallel_processing': True, 'max_cores': 32, 'wrf': {'path_to_data': '/share/D3/data/hweiner/WRF/wrfout_6_19_2013/*', 'is_idealized': False, 'feature_tracking_var': 'DBZ', 'segmentation_var': 'DBZ', 'tobac': {'feature_id': {'threshold': [30, 40, 50], 'target': 'maximum', 'position_threshold': 'weighted_diff', 'sigma_threshold': 0.5, 'n_min_threshold': 4}, 'linking': {'method_linking': 'predict', 'adaptive_stop': 0.2, 'adaptive_step': 0.95, 'order': 1, 'subnetwork_size': 10, 'memory': 0, 'v_max': 20, 'time_cell_min': 120}, 'segmentation_2d': {'height': 2, 'method': 'watershed', 'target': 'maximum', 'threshold': 30}, 'segmentation_3d': {'method': 'watershed', 'target': 'maximum', 'threshold': 30, 'seed_3D_flag': 'box'}, 'analysis': {'merge_split': {'variable': 'DBZ'}, 'eth': {'variable': 'DBZ', 'threshold': 30}, 'area-low': {'height': 2}, 'area-high': {'height': 6}, 'volume': {}, 'max_intensity': {'variable': 'DBZ'}, 'velocity': {}, 'perimeter': {}, 'cell_growth': {}, 'irregularity': {'irregularity_metrics': ['convexity', 'sphericity']}}}, 'tams': {'ctt_threshold': 235, 'ctt_core_threshold': 219, 'u_projection': 0, 'parallel': False, 'analysis_type': 'cloud'}}} \n",
      "\n",
      "=====Loading WRF Data=====\n",
      "=====Starting WRF tobac Feature ID=====\n",
      "=====Starting WRF tobac Feature Linking=====\n",
      "=====Starting WRF tobac 2D Segmentation=====\n",
      "=====Starting WRF tobac 3D Segmentation=====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=====Performing tobac Linking to UDAF====: 100%|██████████| 15756/15756 [00:12<00:00, 1223.02it/s]\n",
      "=====Performing tobac Segmentation to UDAF=====: 100%|██████████| 25/25 [00:08<00:00,  3.12it/s]\n",
      "=====Performing tobac Segmentation to UDAF=====: 100%|██████████| 25/25 [16:30<00:00, 39.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Starting WRF tobac Analysis Calculations=====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=====Calculating ETH=====: 100%|██████████| 25/25 [09:40<00:00, 23.21s/it]\n",
      "=====Calculating Cell Perimeters=====: 100%|██████████| 15756/15756 [1:12:36<00:00,  3.62it/s]\n",
      "=====Calculating Volumes=====: 100%|██████████| 25/25 [52:03<00:00, 124.93s/it] \n",
      "=====Calculating Merger Edges=====: 100%|██████████| 5355/5355 [1:16:12<00:00,  1.17it/s]\n",
      "=====Calculating Split Edges=====: 100%|██████████| 5226/5226 [1:09:48<00:00,  1.25it/s]\n",
      "=====Filtering By Touching %=====: 100%|██████████| 10581/10581 [1:19:20<00:00,  2.22it/s] \n",
      "=====Calculating Overlap=====: 100%|██████████| 627/627 [07:10<00:00,  1.46it/s]  \n",
      "=====Creating Output Dataframe=====: 100%|██████████| 562/562 [00:01<00:00, 453.44it/s]\n",
      "=====Calculating Areas=====: 100%|██████████| 25/25 [01:04<00:00,  2.60s/it]\n",
      "=====Calculating Areas=====: 100%|██████████| 25/25 [01:04<00:00,  2.57s/it]\n",
      "=====Calculating Max Intensity=====: 100%|██████████| 15756/15756 [1:37:09<00:00,  2.70it/s]\n",
      "=====Calculating Cell Velocities=====: 100%|██████████| 15756/15756 [10:22<00:00, 25.30it/s]\n",
      "=====Calculating Cell Growth Rates=====: 100%|██████████| 6048/6048 [00:03<00:00, 1827.50it/s]\n",
      "=====Calculating Cell Convexities=====: 100%|██████████| 25/25 [59:38<00:00, 143.16s/it] \n",
      "=====Calculating Cell Sphericities=====: 100%|██████████| 15756/15756 [00:00<00:00, 178028.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Converting WRF tobac Output to CoMET-UDAF=====\n",
      "=====WRF tobac Tracking Complete=====\n",
      "=====Starting WRF TAMS Tracking=====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=====Calculating WRF Brightness Temperatures=====: 100%|██████████| 34687625/34687625 [02:04<00:00, 278236.04it/s]\n",
      "=====Calculating WRF Precipitation Rate=====: 100%|██████████| 24/24 [00:00<00:00, 143.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting `identify` 2024-12-04 00:27:40\n",
      "Starting `track` 2024-12-04 00:27:55\n",
      "Starting `classify` 2024-12-04 00:27:56\n",
      "=====Converting TAMS Dataframe to Mask=====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=====Converting TAMS to UDAF=====: 100%|██████████| 103/103 [00:02<00:00, 40.88it/s]\n",
      "=====Processing TAMS Cell Lifetimes====: 100%|██████████| 103/103 [00:01<00:00, 55.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====WRF TAMS Tracking Complete=====\n",
      "=====CoMET Performance Diagonistics=====\n",
      "$ Total Process Time: 34463.65 Seconds\n",
      "$ Allocated Resources: Cores = 32, Threads = 64\n"
     ]
    }
   ],
   "source": [
    "import CoMET\n",
    "\n",
    "CONFIG = CoMET.CoMET_load(CONFIG_string=CONFIG_str)\n",
    "print(CONFIG, \"\\n\")\n",
    "\n",
    "output = CoMET.CoMET_start(CONFIG=CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now saving wrf data with tobac tracker\n",
      "\t Saving tracks information\n",
      "\t Saving the 3D segmentation information\n",
      "\t Saving the 2D segmentation information\n",
      "\t Now saving analysis variables:\n",
      "\t \t eth\n",
      "\t \t perimeter\n",
      "\t \t volume\n",
      "\t \t merge/split information\n",
      "\t \t area-low\n",
      "\t \t area-high\n",
      "\t \t max_intensity\n",
      "\t \t speed \n",
      "\t \t velocity\n",
      "\t \t cell_growth\n",
      "\t \t convexity\n",
      "\t \t sphericity\n",
      "Now saving wrf data with tams tracker\n",
      "\t Saving tracks information\n",
      "\t Saving the 2D segmentation information\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NpEncoder, self).default(obj)\n",
    "\n",
    "# Make a copy of the outcopy as to not corrupt it when saving\n",
    "outcopy = deepcopy(output)\n",
    "\n",
    "# Define the path where to save the files\n",
    "savepath = \"/share/D3/data/hweiner/CoMET_out/Test_Out\"\n",
    "\n",
    "for dataset in outcopy.keys():\n",
    "\n",
    "    for tracker in outcopy[dataset].keys():\n",
    "        print(f\"Now saving {dataset} data with {tracker} tracker\")\n",
    "\n",
    "        # Save the tracks and segmentation information\n",
    "        tracks_copy = deepcopy(outcopy[dataset][tracker][\"UDAF_tracks\"]) \n",
    "\n",
    "        # the csv cannot hold datetime objects, so transform into seconds\n",
    "        tracks_copy[\"lifetime\"] = tracks_copy[\"lifetime\"].dt.total_seconds()\n",
    "        tracks_copy.to_csv(f\"{savepath}/{dataset.upper()}_Tracks_{tracker}.csv\", index = False)\n",
    "\n",
    "        print(\"\\t Saving tracks information\")\n",
    "\n",
    "        # Save the segmentation if there is any\n",
    "        if \"UDAF_segmentation_3d\" in outcopy[dataset][tracker] and outcopy[dataset][tracker][\"UDAF_segmentation_3d\"] is not None:\n",
    "\n",
    "            segmentation_3d = deepcopy(outcopy[dataset][tracker][\"UDAF_segmentation_3d\"])\n",
    "\n",
    "            with open(f\"{savepath}/{dataset.upper()}_segmentation_3d_{tracker}.pickle\", \"wb\") as file:\n",
    "                pickle.dump(segmentation_3d, file)\n",
    "            \n",
    "            print(\"\\t Saving the 3D segmentation information\")\n",
    "\n",
    "\n",
    "        if \"UDAF_segmentation_2d\" in outcopy[dataset][tracker] and outcopy[dataset][tracker][\"UDAF_segmentation_2d\"] is not None:\n",
    "\n",
    "            segmentation_2d = deepcopy(outcopy[dataset][tracker][\"UDAF_segmentation_2d\"])\n",
    "\n",
    "            with open(f\"{savepath}/{dataset.upper()}_segmentation_2d_{tracker}.pickle\", \"wb\") as file:\n",
    "                pickle.dump(segmentation_2d, file)\n",
    "\n",
    "            print(\"\\t Saving the 2D segmentation information\")\n",
    "\n",
    "\n",
    "        # Save the analysis data in a dictionary\n",
    "        if \"analysis\" in outcopy[dataset][tracker] and len(outcopy[dataset][tracker][\"analysis\"]) != 0:\n",
    "\n",
    "            print(f\"\\t Now saving analysis variables:\")\n",
    "\n",
    "            save_dict = {}\n",
    "\n",
    "            for analysis_variable in outcopy[dataset][tracker][\"analysis\"].keys():\n",
    "\n",
    "                if analysis_variable in [\"area-high\", \"area-low\"]:\n",
    "\n",
    "                    save_dict[f\"{dataset}_{tracker}_{analysis_variable}\"] = outcopy[dataset][tracker][\"analysis\"][analysis_variable][\"area\"].values.tolist()\n",
    "                    print(f\"\\t \\t {analysis_variable}\")\n",
    "\n",
    "                elif analysis_variable in [\"irregularity\"]:\n",
    "                    \n",
    "                    # go through all of the irregularity measurements in analysis\n",
    "                    for irregularity_key in outcopy[dataset][tracker][\"analysis\"][analysis_variable].keys()[3:]: # skip frame, feature id, cell id\n",
    "                        \n",
    "                        save_dict[f\"{dataset}_{tracker}_{irregularity_key}\"] = outcopy[dataset][tracker][\"analysis\"][analysis_variable][irregularity_key].values.tolist()\n",
    "                        print(f\"\\t \\t {irregularity_key}\")\n",
    "\n",
    "                elif analysis_variable in [\"velocity\"]:\n",
    "                    \n",
    "                    # save both the speed and velocity from the velocity analysis dataframe\n",
    "                    save_dict[f\"{dataset}_{tracker}_velocity\"] = outcopy[dataset][tracker][\"analysis\"][analysis_variable][\"velocity\"].values.tolist()\n",
    "                    save_dict[f\"{dataset}_{tracker}_speed\"] = outcopy[dataset][tracker][\"analysis\"][analysis_variable][\"speed\"].values.tolist()\n",
    "                    print(f\"\\t \\t speed \\n\\t \\t velocity\")\n",
    "\n",
    "                elif analysis_variable == \"merge_split\":\n",
    "                    \n",
    "                    save_dict[f\"{dataset}_{tracker}_merger_frame\"] = outcopy[dataset][tracker][\"analysis\"][analysis_variable][0][\"frame\"].values.tolist()\n",
    "                    save_dict[f\"{dataset}_{tracker}_merger_parent_cells\"] = outcopy[dataset][tracker][\"analysis\"][analysis_variable][0][\"parent_cells\"].values.tolist()\n",
    "                    save_dict[f\"{dataset}_{tracker}_merger_merged_cell\"] = outcopy[dataset][tracker][\"analysis\"][analysis_variable][0][\"merged_cell\"].values.tolist()\n",
    "\n",
    "                    save_dict[f\"{dataset}_{tracker}_splitter_frame\"] = outcopy[dataset][tracker][\"analysis\"][analysis_variable][1][\"frame\"].values.tolist()\n",
    "                    save_dict[f\"{dataset}_{tracker}_splitter_split_cell\"] = outcopy[dataset][tracker][\"analysis\"][analysis_variable][1][\"split_cell\"].values.tolist()\n",
    "                    save_dict[f\"{dataset}_{tracker}_splitter_child_cells\"] = outcopy[dataset][tracker][\"analysis\"][analysis_variable][1][\"child_cells\"].values.tolist()\n",
    "                    print(\"\\t \\t merge/split information\")\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    save_dict[f\"{dataset}_{tracker}_{analysis_variable}\"] = outcopy[dataset][tracker][\"analysis\"][analysis_variable][analysis_variable].values.tolist()\n",
    "                    print(f\"\\t \\t {analysis_variable}\")\n",
    "\n",
    "            with open(f\"{savepath}/{dataset.upper()}_AnalysisVariables_{tracker}.json\", \"w\") as outfile:\n",
    "                json.dump(save_dict, outfile, cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "path_to_files = \"/share/D3/data/hweiner/CoMET_out/Test_Out\"\n",
    "\n",
    "with open(f\"{path_to_files}/WRF_AnalysisVariables_tobac.json\", \"r\") as file:\n",
    "    WRF_analysis_data_tobac = json.load(file)\n",
    "\n",
    "WRF_tracks_tobac = pd.read_csv(f\"{path_to_files}/WRF_Tracks_tobac.csv\")\n",
    "WRF_tracks_tams = pd.read_csv(f\"{path_to_files}/WRF_Tracks_tams.csv\")\n",
    "\n",
    "with open(f\"{path_to_files}/WRF_segmentation_2d_tobac.pickle\", \"rb\") as data:\n",
    "    WRF_segmentation_2d_tobac = pickle.load(data)\n",
    "with open(f\"{path_to_files}/WRF_segmentation_2d_tams.pickle\", \"rb\") as data:\n",
    "    WRF_segmentation_2d_tams = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRF_analysis_data_tobac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRF_tracks_tobac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRF_segmentation_2d_tobac"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".CoMET",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
